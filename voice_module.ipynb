{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c414dc4-292b-43b0-a192-e08f52e78177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "funasr version: 1.2.7.\n",
      "Check update of funasr, and it would cost few times. You may disable it by set `disable_update=True` in AutoModel\n",
      "You are using the latest version of funasr-1.2.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 29 files: 100%|█████████████████████| 29/29 [00:00<00:00, 60304.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect model requirements, begin to install it: /Users/madhusiddharthsuthagar/.cache/huggingface/hub/models--FunAudioLLM--SenseVoiceSmall/snapshots/3eb3b4eeffc2f2dde6051b853983753db33e35c3/requirements.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install model requirements successfully\n"
     ]
    }
   ],
   "source": [
    "from funasr import AutoModel\n",
    "\n",
    "model = AutoModel(model=\"FunAudioLLM/SenseVoiceSmall\", hub=\"hf\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce7214b5-4768-4489-9e1c-016d99c26c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at r-f/wav2vec-english-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-2 sec: disgust (confidence: 0.16)\n",
      "2-4 sec: disgust (confidence: 0.15)\n",
      "4-4 sec: sad (confidence: 0.15)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Load model and feature extractor\n",
    "# -----------------------------\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
    ")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Load audio\n",
    "# -----------------------------\n",
    "audio_file = \"/Users/madhusiddharthsuthagar/Downloads/input_9.mp3\"\n",
    "waveform, rate = librosa.load(audio_file, sr=16000)\n",
    "total_duration = len(waveform) / rate\n",
    "\n",
    "# -----------------------------\n",
    "# Split into non-overlapping 3-second chunks\n",
    "# -----------------------------\n",
    "chunk_duration = 2 # seconds\n",
    "chunk_size = int(chunk_duration * rate)\n",
    "num_chunks = math.ceil(len(waveform) / chunk_size)\n",
    "\n",
    "chunks = []\n",
    "chunk_times = []\n",
    "for i in range(num_chunks):\n",
    "    start_sample = i * chunk_size\n",
    "    end_sample = start_sample + chunk_size\n",
    "    chunk = waveform[start_sample:end_sample]\n",
    "    \n",
    "    # Skip silent/very quiet chunks\n",
    "    rms = np.sqrt(np.mean(chunk**2))\n",
    "    if len(chunk) > 0 and rms > 0.01:\n",
    "        chunks.append(chunk)\n",
    "        chunk_times.append((start_sample / rate, min(end_sample / rate, total_duration)))\n",
    "\n",
    "# -----------------------------\n",
    "# Run inference for each chunk\n",
    "# -----------------------------\n",
    "previous_emotion = None\n",
    "confidence_threshold = 0.5  # adjust as needed\n",
    "\n",
    "for start, end, chunk in zip([t[0] for t in chunk_times], [t[1] for t in chunk_times], chunks):\n",
    "    inputs = feature_extractor(chunk, sampling_rate=rate, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    probs = torch.softmax(logits.squeeze(0), dim=-1)\n",
    "    top_prob, predicted_id = torch.max(probs, dim=-1)\n",
    "    emotion = model.config.id2label[predicted_id.item()]\n",
    "\n",
    "    # Only update if confidence is above threshold\n",
    "    if top_prob.item() >= confidence_threshold:\n",
    "        previous_emotion = emotion\n",
    "    elif previous_emotion is not None:\n",
    "        emotion = previous_emotion  # keep last confident emotion\n",
    "\n",
    "    print(f\"{int(start)}-{int(end)} sec: {emotion} (confidence: {top_prob.item():.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5e22054d-09ca-44f6-9c84-d3b1c39f713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting advanced emotion recognition...\n",
      "Loading r-f/wav2vec-english-speech-emotion-recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at r-f/wav2vec-english-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded r-f/wav2vec-english-speech-emotion-recognition\n",
      "Loading ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition were not used when initializing Wav2Vec2ForSequenceClassification: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.output.bias', 'classifier.output.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition\n",
      "Loading speechbrain/emotion-recognition-wav2vec2-IEMOCAP...\n",
      "✗ Failed to load speechbrain/emotion-recognition-wav2vec2-IEMOCAP: Can't load feature extractor for 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP' is the correct path to a directory containing a preprocessor_config.json file\n",
      "Processing audio: 4.38 seconds\n",
      "   0.0-   1.0s: happy        (conf: 0.589)\n",
      "   0.5-   1.5s: happy        (conf: 0.574)\n",
      "   1.0-   2.0s: neutral      (conf: 1.000)\n",
      "   1.5-   2.5s: neutral      (conf: 1.000)\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy._FloatAbstractDType'> could not be promoted by <class 'numpy.dtypes.StrDType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.StrDType'>, <class 'numpy._FloatAbstractDType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDTypePromotionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 262\u001b[39m\n\u001b[32m    259\u001b[39m audio_file = \u001b[33m\"\u001b[39m\u001b[33m/Users/madhusiddharthsuthagar/Downloads/input_9.mp3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting advanced emotion recognition...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m results = \u001b[43mprocess_audio_advanced\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m    265\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEMOTION TIMELINE SUMMARY\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 241\u001b[39m, in \u001b[36mprocess_audio_advanced\u001b[39m\u001b[34m(audio_file)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m confidence < \u001b[32m0.6\u001b[39m:\n\u001b[32m    240\u001b[39m         recent_emotions = emotions_history[-\u001b[32m3\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         emotion = \u001b[43mstats\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecent_emotions\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    242\u001b[39m         confidence = np.mean(confidences_history[-\u001b[32m3\u001b[39m:])\n\u001b[32m    244\u001b[39m emotion_sequence.append({\n\u001b[32m    245\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mstart\u001b[39m\u001b[33m'\u001b[39m: start_time,\n\u001b[32m    246\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mend\u001b[39m\u001b[33m'\u001b[39m: end_time,\n\u001b[32m    247\u001b[39m     \u001b[33m'\u001b[39m\u001b[33memotion\u001b[39m\u001b[33m'\u001b[39m: emotion,\n\u001b[32m    248\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mconfidence\u001b[39m\u001b[33m'\u001b[39m: confidence\n\u001b[32m    249\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sensevoice_env/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:543\u001b[39m, in \u001b[36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator.<locals>.axis_nan_policy_wrapper\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    540\u001b[39m     samples = [sample.reshape(new_shape)\n\u001b[32m    541\u001b[39m                \u001b[38;5;28;01mfor\u001b[39;00m sample, new_shape \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(samples, new_shapes)]\n\u001b[32m    542\u001b[39m axis = -\u001b[32m1\u001b[39m  \u001b[38;5;66;03m# work over the last axis\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m NaN = \u001b[43m_get_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m samples \u001b[38;5;28;01melse\u001b[39;00m np.nan\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# if axis is not needed, just handle nan_policy and return\u001b[39;00m\n\u001b[32m    546\u001b[39m ndims = np.array([sample.ndim \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m samples])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sensevoice_env/lib/python3.12/site-packages/scipy/_lib/_util.py:1023\u001b[39m, in \u001b[36m_get_nan\u001b[39m\u001b[34m(shape, xp, *data)\u001b[39m\n\u001b[32m   1021\u001b[39m xp = array_namespace(*data) \u001b[38;5;28;01mif\u001b[39;00m xp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xp\n\u001b[32m   1022\u001b[39m \u001b[38;5;66;03m# Get NaN of appropriate dtype for data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m dtype = \u001b[43mxp_result_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_floating\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1024\u001b[39m device = xp_result_device(*data)\n\u001b[32m   1025\u001b[39m res = xp.full(shape, xp.nan, dtype=dtype, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sensevoice_env/lib/python3.12/site-packages/scipy/_lib/_array_api.py:533\u001b[39m, in \u001b[36mxp_result_type\u001b[39m\u001b[34m(force_floating, xp, *args)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp) \u001b[38;5;129;01mand\u001b[39;00m xp.__version__ < \u001b[33m'\u001b[39m\u001b[33m2.0\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    530\u001b[39m     \u001b[38;5;66;03m# Follow NEP 50 promotion rules anyway\u001b[39;00m\n\u001b[32m    531\u001b[39m     args_not_none = [arg.dtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[33m'\u001b[39m\u001b[33msize\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0\u001b[39m) == \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[32m    532\u001b[39m                      \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args_not_none]\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_not_none\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# follow library's preferred promotion rules\u001b[39;00m\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.result_type(*args_not_none)\n",
      "\u001b[31mDTypePromotionError\u001b[39m: The DType <class 'numpy._FloatAbstractDType'> could not be promoted by <class 'numpy.dtypes.StrDType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.StrDType'>, <class 'numpy._FloatAbstractDType'>)"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------\n",
    "# Multiple Model Ensemble Approach\n",
    "# -----------------------------\n",
    "\n",
    "class MultiModelEmotionRecognizer:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.feature_extractors = {}\n",
    "        self.model_weights = {}\n",
    "        \n",
    "        # Load multiple models for ensemble\n",
    "        self.load_models()\n",
    "    \n",
    "    def load_models(self):\n",
    "        \"\"\"Load multiple pre-trained models\"\"\"\n",
    "        model_configs = [\n",
    "            {\n",
    "                'name': 'r-f/wav2vec-english-speech-emotion-recognition',\n",
    "                'weight': 0.4\n",
    "            },\n",
    "            {\n",
    "                'name': 'ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition',\n",
    "                'weight': 0.35\n",
    "            },\n",
    "            {\n",
    "                'name': 'speechbrain/emotion-recognition-wav2vec2-IEMOCAP',\n",
    "                'weight': 0.25\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for config in model_configs:\n",
    "            try:\n",
    "                model_name = config['name']\n",
    "                print(f\"Loading {model_name}...\")\n",
    "                \n",
    "                self.feature_extractors[model_name] = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "                self.models[model_name] = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "                self.models[model_name].eval()\n",
    "                self.model_weights[model_name] = config['weight']\n",
    "                \n",
    "                print(f\"✓ Loaded {model_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Failed to load {model_name}: {e}\")\n",
    "    \n",
    "    def extract_advanced_features(self, audio_chunk, sr):\n",
    "        \"\"\"Extract additional features for robustness\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio_chunk, sr=sr)[0]\n",
    "        features['spectral_centroid_mean'] = np.mean(spectral_centroids)\n",
    "        features['spectral_centroid_std'] = np.std(spectral_centroids)\n",
    "        \n",
    "        # Zero crossing rate\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio_chunk)[0]\n",
    "        features['zcr_mean'] = np.mean(zcr)\n",
    "        features['zcr_std'] = np.std(zcr)\n",
    "        \n",
    "        # Energy features\n",
    "        rms_energy = librosa.feature.rms(y=audio_chunk)[0]\n",
    "        features['rms_mean'] = np.mean(rms_energy)\n",
    "        features['rms_std'] = np.std(rms_energy)\n",
    "        \n",
    "        # Pitch features\n",
    "        pitches, magnitudes = librosa.core.piptrack(y=audio_chunk, sr=sr, threshold=0.1)\n",
    "        pitch_values = []\n",
    "        for t in range(pitches.shape[1]):\n",
    "            index = magnitudes[:, t].argmax()\n",
    "            pitch = pitches[index, t]\n",
    "            if pitch > 0:\n",
    "                pitch_values.append(pitch)\n",
    "        \n",
    "        if pitch_values:\n",
    "            features['pitch_mean'] = np.mean(pitch_values)\n",
    "            features['pitch_std'] = np.std(pitch_values)\n",
    "        else:\n",
    "            features['pitch_mean'] = 0\n",
    "            features['pitch_std'] = 0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def is_speech_segment(self, audio_chunk, sr):\n",
    "        \"\"\"Improved speech detection\"\"\"\n",
    "        # Energy-based detection\n",
    "        rms = np.sqrt(np.mean(audio_chunk**2))\n",
    "        if rms < 0.005:  # Very quiet\n",
    "            return False\n",
    "        \n",
    "        # Zero crossing rate (speech typically has moderate ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio_chunk)[0]\n",
    "        zcr_mean = np.mean(zcr)\n",
    "        if zcr_mean > 0.35:  # Too noisy/high-frequency\n",
    "            return False\n",
    "        \n",
    "        # Spectral features\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio_chunk, sr=sr)[0]\n",
    "        spec_centroid_mean = np.mean(spectral_centroids)\n",
    "        if spec_centroid_mean < 500 or spec_centroid_mean > 8000:  # Outside typical speech range\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def predict_emotion(self, audio_chunk, sr):\n",
    "        \"\"\"Predict emotion using ensemble of models\"\"\"\n",
    "        if not self.is_speech_segment(audio_chunk, sr):\n",
    "            return None, 0.0\n",
    "        \n",
    "        predictions = {}\n",
    "        confidences = {}\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for model_name in self.models.keys():\n",
    "            try:\n",
    "                inputs = self.feature_extractors[model_name](\n",
    "                    audio_chunk, \n",
    "                    sampling_rate=sr, \n",
    "                    return_tensors=\"pt\", \n",
    "                    padding=True\n",
    "                )\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = self.models[model_name](**inputs).logits\n",
    "                \n",
    "                probs = torch.softmax(logits.squeeze(0), dim=-1)\n",
    "                top_prob, predicted_id = torch.max(probs, dim=-1)\n",
    "                \n",
    "                emotion = self.models[model_name].config.id2label[predicted_id.item()]\n",
    "                predictions[model_name] = emotion\n",
    "                confidences[model_name] = top_prob.item()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {model_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not predictions:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Weighted ensemble voting\n",
    "        emotion_scores = {}\n",
    "        total_weight = 0\n",
    "        \n",
    "        for model_name, emotion in predictions.items():\n",
    "            weight = self.model_weights[model_name] * confidences[model_name]\n",
    "            if emotion not in emotion_scores:\n",
    "                emotion_scores[emotion] = 0\n",
    "            emotion_scores[emotion] += weight\n",
    "            total_weight += weight\n",
    "        \n",
    "        if total_weight == 0:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Normalize scores\n",
    "        for emotion in emotion_scores:\n",
    "            emotion_scores[emotion] /= total_weight\n",
    "        \n",
    "        # Get final prediction\n",
    "        final_emotion = max(emotion_scores.items(), key=lambda x: x[1])\n",
    "        return final_emotion[0], final_emotion[1]\n",
    "\n",
    "def process_audio_advanced(audio_file):\n",
    "    \"\"\"Advanced audio processing with improved robustness\"\"\"\n",
    "    \n",
    "    # Initialize recognizer\n",
    "    recognizer = MultiModelEmotionRecognizer()\n",
    "    \n",
    "    # Load audio with preprocessing\n",
    "    waveform, rate = librosa.load(audio_file, sr=16000)\n",
    "    \n",
    "    # Audio preprocessing for robustness\n",
    "    # Normalize audio\n",
    "    waveform = librosa.util.normalize(waveform)\n",
    "    \n",
    "    # Remove silence from beginning and end\n",
    "    waveform, _ = librosa.effects.trim(waveform, top_db=20)\n",
    "    \n",
    "    total_duration = len(waveform) / rate\n",
    "    print(f\"Processing audio: {total_duration:.2f} seconds\")\n",
    "    \n",
    "    # Dynamic chunking based on voice activity\n",
    "    chunk_duration = 1  # seconds\n",
    "    overlap_duration = 0.5  # seconds overlap\n",
    "    chunk_size = int(chunk_duration * rate)\n",
    "    overlap_size = int(overlap_duration * rate)\n",
    "    \n",
    "    chunks = []\n",
    "    chunk_times = []\n",
    "    \n",
    "    # Create overlapping chunks\n",
    "    step_size = chunk_size - overlap_size\n",
    "    num_chunks = math.ceil((len(waveform) - chunk_size) / step_size) + 1\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_sample = i * step_size\n",
    "        end_sample = min(start_sample + chunk_size, len(waveform))\n",
    "        \n",
    "        if end_sample - start_sample < chunk_size // 2:  # Skip very short chunks\n",
    "            break\n",
    "            \n",
    "        chunk = waveform[start_sample:end_sample]\n",
    "        \n",
    "        # Pad short chunks\n",
    "        if len(chunk) < chunk_size:\n",
    "            chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode='constant')\n",
    "        \n",
    "        chunks.append(chunk)\n",
    "        chunk_times.append((start_sample / rate, min(end_sample / rate, total_duration)))\n",
    "    \n",
    "    # Emotion tracking with smoothing\n",
    "    emotions_history = []\n",
    "    confidences_history = []\n",
    "    emotion_sequence = []\n",
    "    \n",
    "    # Process each chunk\n",
    "    for i, (start_time, end_time, chunk) in enumerate(zip([t[0] for t in chunk_times], \n",
    "                                                         [t[1] for t in chunk_times], \n",
    "                                                         chunks)):\n",
    "        \n",
    "        emotion, confidence = recognizer.predict_emotion(chunk, rate)\n",
    "        \n",
    "        if emotion is None:\n",
    "            print(f\"{start_time:6.1f}-{end_time:6.1f}s: [SILENCE]\")\n",
    "            continue\n",
    "        \n",
    "        emotions_history.append(emotion)\n",
    "        confidences_history.append(confidence)\n",
    "        \n",
    "        # Temporal smoothing - consider previous predictions\n",
    "        if len(emotions_history) >= 3:\n",
    "            # Use mode of last 3 predictions if confidence is low\n",
    "            if confidence < 0.6:\n",
    "                recent_emotions = emotions_history[-3:]\n",
    "                emotion = stats.mode(recent_emotions)[0]\n",
    "                confidence = np.mean(confidences_history[-3:])\n",
    "        \n",
    "        emotion_sequence.append({\n",
    "            'start': start_time,\n",
    "            'end': end_time,\n",
    "            'emotion': emotion,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "        \n",
    "        print(f\"{start_time:6.1f}-{end_time:6.1f}s: {emotion:12} (conf: {confidence:.3f})\")\n",
    "    \n",
    "    return emotion_sequence\n",
    "\n",
    "# -----------------------------\n",
    "# Usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"/Users/madhusiddharthsuthagar/Downloads/input_9.mp3\"\n",
    "    \n",
    "    print(\"Starting advanced emotion recognition...\")\n",
    "    results = process_audio_advanced(audio_file)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EMOTION TIMELINE SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"{result['start']:6.1f}-{result['end']:6.1f}s: {result['emotion']:12} ({result['confidence']:.3f})\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    if results:\n",
    "        emotions = [r['emotion'] for r in results]\n",
    "        confidences = [r['confidence'] for r in results]\n",
    "        \n",
    "        print(f\"\\nOverall Statistics:\")\n",
    "        print(f\"Average confidence: {np.mean(confidences):.3f}\")\n",
    "        print(f\"Dominant emotion: {stats.mode(emotions)[0]}\")\n",
    "        print(f\"Emotion changes: {len(set(emotions))} different emotions detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "67828214-dca7-47e0-963b-e120d41e3b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at r-f/wav2vec-english-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0-  1 sec: disgust      (conf: 0.15)\n",
      "  0-  1 sec: disgust      (conf: 0.15)\n",
      "  1-  2 sec: angry        (conf: 0.15)\n",
      "  1-  2 sec: disgust      (smoothed, orig: angry, conf: 0.17)\n",
      "  2-  3 sec: disgust      (smoothed, orig: angry, conf: 0.16)\n",
      "  2-  3 sec: disgust      (smoothed, orig: angry, conf: 0.16)\n",
      "  3-  4 sec: disgust      (smoothed, orig: angry, conf: 0.17)\n",
      "  3-  4 sec: disgust      (smoothed, orig: angry, conf: 0.16)\n",
      "  4-  5 sec: disgust      (smoothed, orig: disgust, conf: 0.15)\n",
      "  4-  5 sec: disgust      (smoothed, orig: disgust, conf: 0.15)\n",
      "  5-  6 sec: disgust      (smoothed, orig: disgust, conf: 0.15)\n",
      "  5-  6 sec: disgust      (smoothed, orig: angry, conf: 0.15)\n",
      "  6-  7 sec: disgust      (smoothed, orig: happy, conf: 0.15)\n",
      "  6-  7 sec: disgust      (smoothed, orig: disgust, conf: 0.15)\n",
      "\n",
      "Summary:\n",
      "Most common emotion: disgust\n",
      "Emotion distribution: {'disgust': 13, 'angry': 1}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# Load model and feature extractor\n",
    "# -----------------------------\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
    ")\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"r-f/wav2vec-english-speech-emotion-recognition\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Load audio\n",
    "# -----------------------------\n",
    "audio_file = \"/Users/madhusiddharthsuthagar/Downloads/input_8.mp3\"\n",
    "waveform, rate = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "# Normalize and trim silence\n",
    "waveform = librosa.util.normalize(waveform)\n",
    "waveform, _ = librosa.effects.trim(waveform, top_db=20)\n",
    "\n",
    "total_duration = len(waveform) / rate\n",
    "\n",
    "# -----------------------------\n",
    "# Split into chunks with overlap\n",
    "# -----------------------------\n",
    "chunk_duration = 1  # seconds - longer for better context\n",
    "overlap_duration = 0.5  # seconds overlap\n",
    "chunk_size = int(chunk_duration * rate)\n",
    "overlap_size = int(overlap_duration * rate)\n",
    "\n",
    "chunks = []\n",
    "chunk_times = []\n",
    "\n",
    "# Create overlapping chunks\n",
    "step_size = chunk_size - overlap_size\n",
    "num_chunks = math.ceil((len(waveform) - chunk_size) / step_size) + 1\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    start_sample = i * step_size\n",
    "    end_sample = min(start_sample + chunk_size, len(waveform))\n",
    "    \n",
    "    if end_sample - start_sample < chunk_size // 2:\n",
    "        break\n",
    "    \n",
    "    chunk = waveform[start_sample:end_sample]\n",
    "    \n",
    "    # Pad short chunks\n",
    "    if len(chunk) < chunk_size:\n",
    "        chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode='constant')\n",
    "    \n",
    "    # Better speech detection\n",
    "    rms = np.sqrt(np.mean(chunk**2))\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(chunk)[0])\n",
    "    \n",
    "    # Skip if too quiet or too noisy\n",
    "    if rms > 0.008 and zcr < 0.3:\n",
    "        chunks.append(chunk)\n",
    "        chunk_times.append((start_sample / rate, min(end_sample / rate, total_duration)))\n",
    "\n",
    "# -----------------------------\n",
    "# Run inference for each chunk with smoothing\n",
    "# -----------------------------\n",
    "emotion_history = []\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "for i, (start, end, chunk) in enumerate(zip([t[0] for t in chunk_times], [t[1] for t in chunk_times], chunks)):\n",
    "    inputs = feature_extractor(chunk, sampling_rate=rate, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    probs = torch.softmax(logits.squeeze(0), dim=-1)\n",
    "    top_prob, predicted_id = torch.max(probs, dim=-1)\n",
    "    emotion = model.config.id2label[predicted_id.item()]\n",
    "    confidence = top_prob.item()\n",
    "    \n",
    "    # Temporal smoothing - use most common emotion from recent history if confidence is low\n",
    "    if len(emotion_history) >= 3 and confidence < confidence_threshold:\n",
    "        recent_emotions = emotion_history[-3:]\n",
    "        emotion_counts = Counter(recent_emotions)\n",
    "        smoothed_emotion = emotion_counts.most_common(1)[0][0]\n",
    "        print(f\"{int(start):3d}-{int(end):3d} sec: {smoothed_emotion:12} (smoothed, orig: {emotion}, conf: {confidence:.2f})\")\n",
    "        emotion = smoothed_emotion\n",
    "    else:\n",
    "        print(f\"{int(start):3d}-{int(end):3d} sec: {emotion:12} (conf: {confidence:.2f})\")\n",
    "    \n",
    "    emotion_history.append(emotion)\n",
    "\n",
    "# Summary\n",
    "if emotion_history:\n",
    "    emotion_counts = Counter(emotion_history)\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Most common emotion: {emotion_counts.most_common(1)[0][0]}\")\n",
    "    print(f\"Emotion distribution: {dict(emotion_counts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4ac18f5f-7c0d-4d51-85c8-82e389c3a336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at r-f/wav2vec-english-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio: 7.36 seconds\n",
      "   0.0-   1.0s: sad          (conf: 0.154)\n",
      "   0.5-   1.5s: sad          (conf: 0.153) [CONSISTENT]\n",
      "   1.0-   2.0s: angry        (conf: 0.081) [CHANGE from sad]\n",
      "   1.5-   2.5s: fear         (conf: 0.117) [CHANGE from angry]\n",
      "   2.0-   3.0s: angry        (conf: 0.157) [CHANGE from fear]\n",
      "   2.5-   3.5s: fear         (conf: 0.192) [CHANGE from angry]\n",
      "   3.0-   4.0s: fear         (conf: 0.272) [CHANGE from fear]\n",
      "   3.5-   4.5s: angry        (conf: 0.158) [CHANGE from fear]\n",
      "   4.0-   5.0s: sad          (conf: 0.088) [CHANGE from angry]\n",
      "   4.5-   5.5s: sad          (conf: 0.193) [CHANGE from sad]\n",
      "   5.0-   6.0s: sad          (conf: 0.271) [CONSISTENT]\n",
      "   5.5-   6.5s: angry        (conf: 0.079) [CHANGE from sad]\n",
      "   6.0-   7.0s: sad          (conf: 0.243) [CHANGE from angry]\n",
      "   6.5-   7.4s: sad          (conf: 0.272) [CHANGE from sad]\n",
      "\n",
      "============================================================\n",
      "TEMPORAL EMOTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "EMOTION PHASES:\n",
      "   0.0-   1.5s ( 1.5s): sad          (avg conf: 0.153)\n",
      "   1.0-   2.0s ( 1.0s): angry        (avg conf: 0.081)\n",
      "   1.5-   2.5s ( 1.0s): fear         (avg conf: 0.117)\n",
      "   2.0-   3.0s ( 1.0s): angry        (avg conf: 0.157)\n",
      "   2.5-   4.0s ( 1.5s): fear         (avg conf: 0.232)\n",
      "   3.5-   4.5s ( 1.0s): angry        (avg conf: 0.158)\n",
      "   4.0-   6.0s ( 2.0s): sad          (avg conf: 0.184)\n",
      "   5.5-   6.5s ( 1.0s): angry        (avg conf: 0.079)\n",
      "   6.0-   7.4s ( 1.4s): sad          (avg conf: 0.258)\n",
      "\n",
      "OVERALL EMOTION DISTRIBUTION:\n",
      "sad         :   4.9s ( 66.0%)\n",
      "angry       :   4.0s ( 54.3%)\n",
      "fear        :   2.5s ( 34.0%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# -----------------------------\n",
    "# Advanced Emotion Recognition with Better Temporal Logic\n",
    "# -----------------------------\n",
    "\n",
    "class AdaptiveEmotionRecognizer:\n",
    "    def __init__(self, model_name=\"r-f/wav2vec-english-speech-emotion-recognition\"):\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Adaptive parameters\n",
    "        self.high_confidence_threshold = 0.7\n",
    "        self.medium_confidence_threshold = 0.5\n",
    "        self.low_confidence_threshold = 0.3\n",
    "        \n",
    "        # Temporal state\n",
    "        self.recent_predictions = deque(maxlen=5)  # Keep last 5 predictions\n",
    "        self.confidence_history = deque(maxlen=5)\n",
    "        \n",
    "        # Emotion transition probabilities (learned from data)\n",
    "        # These help determine if an emotion change is realistic\n",
    "        self.transition_weights = {\n",
    "            'happy': {'happy': 1.0, 'neutral': 0.8, 'excited': 0.9, 'sad': 0.2, 'angry': 0.1},\n",
    "            'sad': {'sad': 1.0, 'neutral': 0.7, 'happy': 0.3, 'angry': 0.4, 'fear': 0.6},\n",
    "            'angry': {'angry': 1.0, 'neutral': 0.6, 'sad': 0.5, 'happy': 0.2, 'disgust': 0.7},\n",
    "            'neutral': {'neutral': 1.0, 'happy': 0.8, 'sad': 0.6, 'angry': 0.5, 'fear': 0.4},\n",
    "            'fear': {'fear': 1.0, 'sad': 0.7, 'neutral': 0.6, 'angry': 0.4, 'surprise': 0.8},\n",
    "            'surprise': {'surprise': 1.0, 'happy': 0.8, 'neutral': 0.7, 'fear': 0.5, 'excited': 0.9},\n",
    "            'disgust': {'disgust': 1.0, 'angry': 0.8, 'neutral': 0.6, 'sad': 0.5, 'fear': 0.4}\n",
    "        }\n",
    "    \n",
    "    def is_valid_speech(self, audio_chunk, sr):\n",
    "        \"\"\"Enhanced speech validation\"\"\"\n",
    "        # Energy check\n",
    "        rms = np.sqrt(np.mean(audio_chunk**2))\n",
    "        if rms < 0.005:\n",
    "            return False, \"too_quiet\"\n",
    "        \n",
    "        # Zero crossing rate\n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(audio_chunk)[0])\n",
    "        if zcr > 0.4:\n",
    "            return False, \"too_noisy\"\n",
    "        \n",
    "        # Spectral features for speech detection\n",
    "        try:\n",
    "            mfccs = librosa.feature.mfcc(y=audio_chunk, sr=sr, n_mfcc=13)\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=audio_chunk, sr=sr)[0]\n",
    "            \n",
    "            # Speech typically has certain MFCC patterns\n",
    "            mfcc_mean = np.mean(mfccs[1:4])  # Focus on formant frequencies\n",
    "            spec_centroid_mean = np.mean(spectral_centroid)\n",
    "            \n",
    "            # Heuristic ranges for speech\n",
    "            if spec_centroid_mean < 300 or spec_centroid_mean > 8000:\n",
    "                return False, \"non_speech_spectrum\"\n",
    "                \n",
    "            if abs(mfcc_mean) > 50:  # Extreme MFCC values often indicate non-speech\n",
    "                return False, \"non_speech_mfcc\"\n",
    "                \n",
    "        except:\n",
    "            pass  # If feature extraction fails, continue with basic checks\n",
    "        \n",
    "        return True, \"valid\"\n",
    "    \n",
    "    def get_emotion_confidence_score(self, current_emotion, current_confidence, recent_context):\n",
    "        \"\"\"Calculate adjusted confidence based on temporal context\"\"\"\n",
    "        \n",
    "        if len(recent_context) == 0:\n",
    "            return current_confidence\n",
    "        \n",
    "        # Get recent emotions and confidences\n",
    "        recent_emotions = [pred['emotion'] for pred in recent_context]\n",
    "        recent_confidences = [pred['confidence'] for pred in recent_context]\n",
    "        \n",
    "        # Base confidence adjustment\n",
    "        adjusted_confidence = current_confidence\n",
    "        \n",
    "        # If current prediction is very different from recent trend, reduce confidence\n",
    "        if len(recent_emotions) >= 2:\n",
    "            # Count how many recent predictions match current\n",
    "            matches = sum(1 for e in recent_emotions[-3:] if e == current_emotion)\n",
    "            consistency_bonus = matches / min(3, len(recent_emotions))\n",
    "            \n",
    "            # Apply transition probability\n",
    "            last_emotion = recent_emotions[-1]\n",
    "            if last_emotion in self.transition_weights and current_emotion in self.transition_weights[last_emotion]:\n",
    "                transition_prob = self.transition_weights[last_emotion][current_emotion]\n",
    "                adjusted_confidence *= (0.5 + 0.5 * transition_prob)  # Scale between 0.5-1.0\n",
    "            \n",
    "            # Consistency adjustment\n",
    "            adjusted_confidence = (adjusted_confidence + consistency_bonus * 0.3) / 1.3\n",
    "        \n",
    "        return min(adjusted_confidence, 1.0)\n",
    "    \n",
    "    def predict_with_context(self, audio_chunk, sr):\n",
    "        \"\"\"Predict emotion with temporal context awareness\"\"\"\n",
    "        \n",
    "        # Check if it's valid speech\n",
    "        is_valid, reason = self.is_valid_speech(audio_chunk, sr)\n",
    "        if not is_valid:\n",
    "            return None, 0.0, reason\n",
    "        \n",
    "        # Get model prediction\n",
    "        try:\n",
    "            inputs = self.feature_extractor(audio_chunk, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**inputs).logits\n",
    "            \n",
    "            probs = torch.softmax(logits.squeeze(0), dim=-1)\n",
    "            \n",
    "            # Get top 3 predictions for more nuanced decision making\n",
    "            top3_probs, top3_ids = torch.topk(probs, 3)\n",
    "            top3_emotions = [self.model.config.id2label[idx.item()] for idx in top3_ids]\n",
    "            \n",
    "            primary_emotion = top3_emotions[0]\n",
    "            primary_confidence = top3_probs[0].item()\n",
    "            \n",
    "            # Adjust confidence based on context\n",
    "            context_confidence = self.get_emotion_confidence_score(\n",
    "                primary_emotion, \n",
    "                primary_confidence, \n",
    "                list(self.recent_predictions)\n",
    "            )\n",
    "            \n",
    "            # If primary prediction has low confidence, consider alternatives\n",
    "            if context_confidence < self.medium_confidence_threshold and len(top3_emotions) > 1:\n",
    "                # Check if second choice is more consistent with recent context\n",
    "                secondary_emotion = top3_emotions[1]\n",
    "                secondary_confidence = top3_probs[1].item()\n",
    "                \n",
    "                if len(self.recent_predictions) >= 2:\n",
    "                    recent_emotions = [pred['emotion'] for pred in list(self.recent_predictions)[-3:]]\n",
    "                    secondary_matches = sum(1 for e in recent_emotions if e == secondary_emotion)\n",
    "                    primary_matches = sum(1 for e in recent_emotions if e == primary_emotion)\n",
    "                    \n",
    "                    # If secondary emotion is much more consistent, use it\n",
    "                    if secondary_matches > primary_matches and secondary_confidence > 0.3:\n",
    "                        return secondary_emotion, secondary_confidence, \"secondary_choice\"\n",
    "            \n",
    "            return primary_emotion, context_confidence, \"primary\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, 0.0, f\"prediction_error: {e}\"\n",
    "    \n",
    "    def process_audio(self, audio_file):\n",
    "        \"\"\"Process entire audio file with adaptive temporal modeling\"\"\"\n",
    "        \n",
    "        # Load and preprocess audio\n",
    "        waveform, rate = librosa.load(audio_file, sr=16000)\n",
    "        waveform = librosa.util.normalize(waveform)\n",
    "        waveform, _ = librosa.effects.trim(waveform, top_db=20)\n",
    "        \n",
    "        total_duration = len(waveform) / rate\n",
    "        print(f\"Processing audio: {total_duration:.2f} seconds\")\n",
    "        \n",
    "        # Adaptive chunking - smaller chunks for more granular detection\n",
    "        chunk_duration = 1  # Shorter chunks\n",
    "        overlap_duration = 0.5\n",
    "        chunk_size = int(chunk_duration * rate)\n",
    "        overlap_size = int(overlap_duration * rate)\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_times = []\n",
    "        \n",
    "        step_size = chunk_size - overlap_size\n",
    "        num_chunks = math.ceil((len(waveform) - chunk_size) / step_size) + 1\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_sample = i * step_size\n",
    "            end_sample = min(start_sample + chunk_size, len(waveform))\n",
    "            \n",
    "            if end_sample - start_sample < chunk_size // 3:  # More lenient minimum chunk size\n",
    "                break\n",
    "            \n",
    "            chunk = waveform[start_sample:end_sample]\n",
    "            \n",
    "            if len(chunk) < chunk_size:\n",
    "                chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode='constant')\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            chunk_times.append((start_sample / rate, min(end_sample / rate, total_duration)))\n",
    "        \n",
    "        # Process chunks\n",
    "        results = []\n",
    "        \n",
    "        for i, (start_time, end_time, chunk) in enumerate(zip([t[0] for t in chunk_times], \n",
    "                                                             [t[1] for t in chunk_times], \n",
    "                                                             chunks)):\n",
    "            \n",
    "            emotion, confidence, reason = self.predict_with_context(chunk, rate)\n",
    "            \n",
    "            if emotion is None:\n",
    "                print(f\"{start_time:6.1f}-{end_time:6.1f}s: [SKIP: {reason}]\")\n",
    "                continue\n",
    "            \n",
    "            # Add to recent predictions for context\n",
    "            prediction = {\n",
    "                'emotion': emotion,\n",
    "                'confidence': confidence,\n",
    "                'start': start_time,\n",
    "                'end': end_time,\n",
    "                'reason': reason\n",
    "            }\n",
    "            \n",
    "            self.recent_predictions.append(prediction)\n",
    "            self.confidence_history.append(confidence)\n",
    "            \n",
    "            # Display with context info\n",
    "            context_info = \"\"\n",
    "            if len(self.recent_predictions) > 1:\n",
    "                recent_emotions = [p['emotion'] for p in list(self.recent_predictions)[-3:]]\n",
    "                if len(set(recent_emotions)) == 1:\n",
    "                    context_info = \" [CONSISTENT]\"\n",
    "                else:\n",
    "                    context_info = f\" [CHANGE from {self.recent_predictions[-2]['emotion']}]\"\n",
    "            \n",
    "            print(f\"{start_time:6.1f}-{end_time:6.1f}s: {emotion:12} (conf: {confidence:.3f}){context_info}\")\n",
    "            \n",
    "            results.append(prediction)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# -----------------------------\n",
    "# Usage\n",
    "# -----------------------------\n",
    "def analyze_audio(audio_file):\n",
    "    recognizer = AdaptiveEmotionRecognizer()\n",
    "    results = recognizer.process_audio(audio_file)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid speech segments detected.\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEMPORAL EMOTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Segment the timeline into emotion phases\n",
    "    emotion_phases = []\n",
    "    current_phase = None\n",
    "    \n",
    "    for result in results:\n",
    "        if current_phase is None or current_phase['emotion'] != result['emotion']:\n",
    "            if current_phase is not None:\n",
    "                emotion_phases.append(current_phase)\n",
    "            \n",
    "            current_phase = {\n",
    "                'emotion': result['emotion'],\n",
    "                'start': result['start'],\n",
    "                'end': result['end'],\n",
    "                'confidences': [result['confidence']],\n",
    "                'duration': 0\n",
    "            }\n",
    "        else:\n",
    "            current_phase['end'] = result['end']\n",
    "            current_phase['confidences'].append(result['confidence'])\n",
    "    \n",
    "    if current_phase is not None:\n",
    "        emotion_phases.append(current_phase)\n",
    "    \n",
    "    # Calculate durations and display phases\n",
    "    print(\"\\nEMOTION PHASES:\")\n",
    "    for phase in emotion_phases:\n",
    "        phase['duration'] = phase['end'] - phase['start']\n",
    "        avg_confidence = np.mean(phase['confidences'])\n",
    "        print(f\"{phase['start']:6.1f}-{phase['end']:6.1f}s ({phase['duration']:4.1f}s): {phase['emotion']:12} (avg conf: {avg_confidence:.3f})\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_duration = results[-1]['end'] - results[0]['start']\n",
    "    emotion_durations = {}\n",
    "    \n",
    "    for phase in emotion_phases:\n",
    "        emotion = phase['emotion']\n",
    "        if emotion not in emotion_durations:\n",
    "            emotion_durations[emotion] = 0\n",
    "        emotion_durations[emotion] += phase['duration']\n",
    "    \n",
    "    print(f\"\\nOVERALL EMOTION DISTRIBUTION:\")\n",
    "    for emotion, duration in sorted(emotion_durations.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (duration / total_duration) * 100\n",
    "        print(f\"{emotion:12}: {duration:5.1f}s ({percentage:5.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"/Users/madhusiddharthsuthagar/Downloads/input_8.mp3\"\n",
    "    results = analyze_audio(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "269e4941-46c8-43aa-b2fc-48b54d28fc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at r-f/wav2vec-english-speech-emotion-recognition and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ensemble emotion recognition (5 runs per chunk)...\n",
      "Processing audio: 5.86 seconds\n",
      "Processing chunk 11/11... \n",
      "Ensemble processing complete!\n",
      "\n",
      "============================================================\n",
      "ENSEMBLE EMOTION ANALYSIS RESULTS\n",
      "============================================================\n",
      "\n",
      "Detected 8 emotion phases:\n",
      "------------------------------------------------------------\n",
      "Phase 1:    0.0-   1.0s ( 1.0s)\n",
      "         Emotion: angry        (avg confidence: 0.152)\n",
      "\n",
      "Phase 2:    0.5-   1.5s ( 1.0s)\n",
      "         Emotion: neutral      (avg confidence: 0.155)\n",
      "\n",
      "Phase 3:    1.0-   2.0s ( 1.0s)\n",
      "         Emotion: disgust      (avg confidence: 0.161)\n",
      "\n",
      "Phase 4:    1.5-   3.0s ( 1.5s)\n",
      "         Emotion: angry        (avg confidence: 0.156)\n",
      "\n",
      "Phase 5:    2.5-   4.0s ( 1.5s)\n",
      "         Emotion: disgust      (avg confidence: 0.159)\n",
      "\n",
      "Phase 6:    3.5-   5.0s ( 1.5s)\n",
      "         Emotion: happy        (avg confidence: 0.151)\n",
      "\n",
      "Phase 7:    4.5-   5.5s ( 1.0s)\n",
      "         Emotion: neutral      (avg confidence: 0.152)\n",
      "\n",
      "Phase 8:    5.0-   5.9s ( 0.9s)\n",
      "         Emotion: angry        (avg confidence: 0.155)\n",
      "\n",
      "OVERALL EMOTION DISTRIBUTION:\n",
      "----------------------------------------\n",
      "angry       :   3.4s ( 57.3%)\n",
      "disgust     :   2.5s ( 42.7%)\n",
      "neutral     :   2.0s ( 34.2%)\n",
      "happy       :   1.5s ( 25.6%)\n",
      "\n",
      "SUMMARY STATISTICS:\n",
      "Total analyzed duration: 5.9s\n",
      "Number of emotion phases: 8\n",
      "Average ensemble confidence: 0.155\n",
      "Most dominant emotion: angry\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import torch\n",
    "import librosa\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class EnsembleEmotionRecognizer:\n",
    "    def __init__(self, model_name=\"r-f/wav2vec-english-speech-emotion-recognition\", num_runs=5):\n",
    "        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        self.num_runs = num_runs\n",
    "        \n",
    "        # Confidence thresholds\n",
    "        self.high_confidence_threshold = 0.7\n",
    "        self.medium_confidence_threshold = 0.5\n",
    "    \n",
    "    def is_valid_speech(self, audio_chunk, sr):\n",
    "        \"\"\"Enhanced speech validation\"\"\"\n",
    "        rms = np.sqrt(np.mean(audio_chunk**2))\n",
    "        if rms < 0.005:\n",
    "            return False\n",
    "        \n",
    "        zcr = np.mean(librosa.feature.zero_crossing_rate(audio_chunk)[0])\n",
    "        if zcr > 0.4:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            spectral_centroid = librosa.feature.spectral_centroid(y=audio_chunk, sr=sr)[0]\n",
    "            spec_centroid_mean = np.mean(spectral_centroid)\n",
    "            \n",
    "            if spec_centroid_mean < 300 or spec_centroid_mean > 8000:\n",
    "                return False\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def predict_single_chunk(self, audio_chunk, sr):\n",
    "        \"\"\"Single prediction for a chunk\"\"\"\n",
    "        if not self.is_valid_speech(audio_chunk, sr):\n",
    "            return None, 0.0\n",
    "        \n",
    "        try:\n",
    "            inputs = self.feature_extractor(audio_chunk, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**inputs).logits\n",
    "            \n",
    "            probs = torch.softmax(logits.squeeze(0), dim=-1)\n",
    "            top_prob, predicted_id = torch.max(probs, dim=-1)\n",
    "            \n",
    "            emotion = self.model.config.id2label[predicted_id.item()]\n",
    "            confidence = top_prob.item()\n",
    "            \n",
    "            return emotion, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            return None, 0.0\n",
    "    \n",
    "    def predict_chunk_ensemble(self, audio_chunk, sr):\n",
    "        \"\"\"Run multiple predictions and return majority vote\"\"\"\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        # Run multiple predictions\n",
    "        for _ in range(self.num_runs):\n",
    "            emotion, confidence = self.predict_single_chunk(audio_chunk, sr)\n",
    "            if emotion is not None:\n",
    "                predictions.append(emotion)\n",
    "                confidences.append(confidence)\n",
    "        \n",
    "        if not predictions:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Majority voting\n",
    "        emotion_counts = Counter(predictions)\n",
    "        most_common = emotion_counts.most_common(1)[0]\n",
    "        final_emotion = most_common[0]\n",
    "        vote_count = most_common[1]\n",
    "        \n",
    "        # Calculate ensemble confidence\n",
    "        # Weight by how many runs agreed and their average confidence\n",
    "        emotion_indices = [i for i, e in enumerate(predictions) if e == final_emotion]\n",
    "        emotion_confidences = [confidences[i] for i in emotion_indices]\n",
    "        \n",
    "        # Ensemble confidence = (vote_ratio * avg_confidence_of_votes)\n",
    "        vote_ratio = vote_count / len(predictions)\n",
    "        avg_confidence = np.mean(emotion_confidences)\n",
    "        ensemble_confidence = vote_ratio * avg_confidence\n",
    "        \n",
    "        return final_emotion, ensemble_confidence\n",
    "    \n",
    "    def process_audio(self, audio_file):\n",
    "        \"\"\"Process audio with ensemble predictions\"\"\"\n",
    "        print(f\"Running ensemble emotion recognition ({self.num_runs} runs per chunk)...\")\n",
    "        \n",
    "        # Load and preprocess audio\n",
    "        waveform, rate = librosa.load(audio_file, sr=16000)\n",
    "        waveform = librosa.util.normalize(waveform)\n",
    "        waveform, _ = librosa.effects.trim(waveform, top_db=20)\n",
    "        \n",
    "        total_duration = len(waveform) / rate\n",
    "        print(f\"Processing audio: {total_duration:.2f} seconds\")\n",
    "        \n",
    "        # Chunking parameters\n",
    "        chunk_duration = 1.0\n",
    "        overlap_duration = 0.5\n",
    "        chunk_size = int(chunk_duration * rate)\n",
    "        overlap_size = int(overlap_duration * rate)\n",
    "        \n",
    "        chunks = []\n",
    "        chunk_times = []\n",
    "        \n",
    "        step_size = chunk_size - overlap_size\n",
    "        num_chunks = math.ceil((len(waveform) - chunk_size) / step_size) + 1\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_sample = i * step_size\n",
    "            end_sample = min(start_sample + chunk_size, len(waveform))\n",
    "            \n",
    "            if end_sample - start_sample < chunk_size // 3:\n",
    "                break\n",
    "            \n",
    "            chunk = waveform[start_sample:end_sample]\n",
    "            \n",
    "            if len(chunk) < chunk_size:\n",
    "                chunk = np.pad(chunk, (0, chunk_size - len(chunk)), mode='constant')\n",
    "            \n",
    "            chunks.append(chunk)\n",
    "            chunk_times.append((start_sample / rate, min(end_sample / rate, total_duration)))\n",
    "        \n",
    "        # Process chunks with ensemble\n",
    "        results = []\n",
    "        total_chunks = len(chunks)\n",
    "        \n",
    "        for i, (start_time, end_time, chunk) in enumerate(zip([t[0] for t in chunk_times], \n",
    "                                                             [t[1] for t in chunk_times], \n",
    "                                                             chunks)):\n",
    "            \n",
    "            print(f\"\\rProcessing chunk {i+1}/{total_chunks}... \", end='', flush=True)\n",
    "            \n",
    "            emotion, confidence = self.predict_chunk_ensemble(chunk, rate)\n",
    "            \n",
    "            if emotion is None:\n",
    "                continue\n",
    "            \n",
    "            result = {\n",
    "                'emotion': emotion,\n",
    "                'confidence': confidence,\n",
    "                'start': start_time,\n",
    "                'end': end_time\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        print(\"\\nEnsemble processing complete!\")\n",
    "        return results\n",
    "\n",
    "def analyze_audio_ensemble(audio_file, num_runs=5):\n",
    "    \"\"\"Main function to analyze audio with ensemble approach\"\"\"\n",
    "    recognizer = EnsembleEmotionRecognizer(num_runs=num_runs)\n",
    "    results = recognizer.process_audio(audio_file)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid speech segments detected.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENSEMBLE EMOTION ANALYSIS RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Group consecutive same emotions into phases\n",
    "    emotion_phases = []\n",
    "    current_phase = None\n",
    "    \n",
    "    for result in results:\n",
    "        if current_phase is None or current_phase['emotion'] != result['emotion']:\n",
    "            if current_phase is not None:\n",
    "                emotion_phases.append(current_phase)\n",
    "            \n",
    "            current_phase = {\n",
    "                'emotion': result['emotion'],\n",
    "                'start': result['start'],\n",
    "                'end': result['end'],\n",
    "                'confidences': [result['confidence']],\n",
    "            }\n",
    "        else:\n",
    "            current_phase['end'] = result['end']\n",
    "            current_phase['confidences'].append(result['confidence'])\n",
    "    \n",
    "    if current_phase is not None:\n",
    "        emotion_phases.append(current_phase)\n",
    "    \n",
    "    # Display emotion phases\n",
    "    print(f\"\\nDetected {len(emotion_phases)} emotion phases:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i, phase in enumerate(emotion_phases, 1):\n",
    "        duration = phase['end'] - phase['start']\n",
    "        avg_confidence = np.mean(phase['confidences'])\n",
    "        \n",
    "        print(f\"Phase {i}: {phase['start']:6.1f}-{phase['end']:6.1f}s ({duration:4.1f}s)\")\n",
    "        print(f\"         Emotion: {phase['emotion']:12} (avg confidence: {avg_confidence:.3f})\")\n",
    "        print()\n",
    "    \n",
    "    # Overall emotion distribution\n",
    "    total_duration = results[-1]['end'] - results[0]['start']\n",
    "    emotion_durations = {}\n",
    "    \n",
    "    for phase in emotion_phases:\n",
    "        emotion = phase['emotion']\n",
    "        duration = phase['end'] - phase['start']\n",
    "        if emotion not in emotion_durations:\n",
    "            emotion_durations[emotion] = 0\n",
    "        emotion_durations[emotion] += duration\n",
    "    \n",
    "    print(\"OVERALL EMOTION DISTRIBUTION:\")\n",
    "    print(\"-\" * 40)\n",
    "    for emotion, duration in sorted(emotion_durations.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = (duration / total_duration) * 100\n",
    "        print(f\"{emotion:12}: {duration:5.1f}s ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    all_confidences = [r['confidence'] for r in results]\n",
    "    print(f\"\\nSUMMARY STATISTICS:\")\n",
    "    print(f\"Total analyzed duration: {total_duration:.1f}s\")\n",
    "    print(f\"Number of emotion phases: {len(emotion_phases)}\")\n",
    "    print(f\"Average ensemble confidence: {np.mean(all_confidences):.3f}\")\n",
    "    print(f\"Most dominant emotion: {max(emotion_durations.items(), key=lambda x: x[1])[0]}\")\n",
    "    \n",
    "    return {\n",
    "        'phases': emotion_phases,\n",
    "        'distribution': emotion_durations,\n",
    "        'total_duration': total_duration,\n",
    "        'avg_confidence': np.mean(all_confidences)\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"/Users/madhusiddharthsuthagar/Downloads/input_1.wav\"\n",
    "    \n",
    "    # Run ensemble analysis (5 predictions per chunk, majority vote)\n",
    "    results = analyze_audio_ensemble(audio_file, num_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad585938-8ce7-4d06-8d30-72c26a8ff068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SenseVoice)",
   "language": "python",
   "name": "sensevoice_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
